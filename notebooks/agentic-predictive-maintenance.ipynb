{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902a7186-2e68-403e-9622-84053bfbafc8",
   "metadata": {},
   "source": [
    "# Agentic Predictive Maintenance for Insured Assets\n",
    "---\n",
    "**Objective:** Develop an agentic predictive maintenance solution using time-series sensor data.\n",
    "\n",
    "**Core Components:**\n",
    "* **Data:** Time-series sensor data, policy data, and maintenance logs/manuals.\n",
    "* **Model:** Time-aware model selection (forecasting/classification) and hyperparameter tuning.\n",
    "* **Agent:** An AI agent combining deterministic checks, RAG (retrieval from manuals), and LLM reasoning to provide explainable maintenance recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1595f0-300c-4c55-ba17-de5fd83a13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../agentic_pm/data_ingest.py\n",
    "\"\"\"\n",
    "agentic_pm/data_ingest.py\n",
    "\n",
    "Purpose:\n",
    "- Load CMAPSS FD001-FD004 (expects files already downloaded into data/raw/CMAPSS)\n",
    "- Parse files into pandas DataFrames with meaningful column names\n",
    "- Compute RUL for train and test sets (per-row RUL for test by merging RUL_FD00X.txt)\n",
    "- Provide EDA helpers (summary stats, sensor/time plots)\n",
    "- Implement normalization strategies:\n",
    "    - global_standardize (global StandardScaler)\n",
    "    - conditional_standardize (cluster by operating-settings and standardize per-cluster)\n",
    "- Save processed CSVs and fitted scalers for later pipeline stages\n",
    "\n",
    "Usage (example):\n",
    ">>> python -m agentic_pm.data_ingest\n",
    "\n",
    "Note: This module does NOT download CMAPSS automatically. Place files like\n",
    "  data/raw/CMAPSS/train_FD001.txt\n",
    "  data/raw/CMAPSS/test_FD001.txt\n",
    "  data/raw/CMAPSS/RUL_FD001.txt\n",
    "\n",
    "Dependencies: pandas, numpy, scikit-learn, matplotlib, seaborn, joblib\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# --- Config\n",
    "RAW_BASE = Path(\"data/raw/CMAPSS\")\n",
    "PROCESSED_BASE = Path(\"data/processed/CMAPSS\")\n",
    "SCALER_DIR = Path(\"artifacts/scalers\")\n",
    "PROCESSED_BASE.mkdir(parents=True, exist_ok=True)\n",
    "SCALER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Column names for CMAPSS files (26 cols)\n",
    "OP_COLS = [\"op_setting_1\", \"op_setting_2\", \"op_setting_3\"]\n",
    "SENSOR_COLS = [f\"sensor_{i}\" for i in range(1, 22)]\n",
    "COL_NAMES = [\"unit\", \"cycle\"] + OP_COLS + SENSOR_COLS\n",
    "\n",
    "# Mapping for normalization strategy per subset\n",
    "# FD001, FD003 -> global; FD002, FD004 -> conditional\n",
    "NORMALIZATION_MAP = {\n",
    "    \"FD001\": \"global\",\n",
    "    \"FD002\": \"conditional\",\n",
    "    \"FD003\": \"global\",\n",
    "    \"FD004\": \"conditional\",\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core loading / parsing\n",
    "# ------------------------\n",
    "\n",
    "def read_cmapps_file(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a CMAPSS train or test txt file into DataFrame with proper column names.\"\"\"\n",
    "    df = pd.read_csv(filepath, sep=\"\\s+\", header=None, names=COL_NAMES)\n",
    "    # ensure dtypes\n",
    "    df[\"unit\"] = df[\"unit\"].astype(int)\n",
    "    df[\"cycle\"] = df[\"cycle\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_rul_file(filepath: Path) -> pd.Series:\n",
    "    \"\"\"Read RUL file (single column) and return a Series indexed by unit (1-based).\"\"\"\n",
    "    rul = pd.read_csv(filepath, header=None, squeeze=True)\n",
    "    rul.index = np.arange(1, len(rul) + 1)  # unit ids are 1..N\n",
    "    rul.name = \"RUL\"\n",
    "    return rul\n",
    "\n",
    "\n",
    "def compute_train_rul(df_train: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add RUL column to train DataFrame: RUL = max_cycle_for_unit - cycle\"\"\"\n",
    "    max_cycle = df_train.groupby(\"unit\")[\"cycle\"].transform(\"max\")\n",
    "    df = df_train.copy()\n",
    "    df[\"RUL\"] = max_cycle - df[\"cycle\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_test_rul(df_test: pd.DataFrame, rul_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Compute per-row RUL for test set by merging per-unit RUL (remaining after last observed cycle).\n",
    "\n",
    "    For each unit:\n",
    "        final_cycle = max(cycle observed in test for that unit)\n",
    "        RUL_at_final = rul_series.loc[unit]\n",
    "    For a row with cycle c:\n",
    "        row_RUL = RUL_at_final + (final_cycle - c)\n",
    "\n",
    "    Returns a new DataFrame with RUL column.\n",
    "    \"\"\"\n",
    "    df = df_test.copy()\n",
    "    final_cycles = df.groupby(\"unit\")[\"cycle\"].transform(\"max\")\n",
    "    # Map per-unit remaining life at last observed cycle\n",
    "    df = df.merge(rul_series.rename(\"RUL_unit\"), left_on=\"unit\", right_index=True)\n",
    "    df[\"RUL\"] = df[\"RUL_unit\"] + (final_cycles - df[\"cycle\"])\n",
    "    df.drop(columns=[\"RUL_unit\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Normalization methods\n",
    "# ------------------------\n",
    "\n",
    "def global_standardize(train_df: pd.DataFrame, test_df: pd.DataFrame, cols: list = None, subset_name: str = \"FD\"):\n",
    "    \"\"\"Fit StandardScaler on train_df[cols] (global) and transform both train and test.\n",
    "    Saves scaler to artifacts/scalers/{subset_name}_global_scaler.pkl\n",
    "    Returns transformed (train, test) and scaler.\n",
    "    \"\"\"\n",
    "    if cols is None:\n",
    "        cols = SENSOR_COLS + OP_COLS\n",
    "    scaler = StandardScaler()\n",
    "    X_train = train_df[cols].values\n",
    "    X_test = test_df[cols].values\n",
    "    scaler.fit(X_train)\n",
    "    train_t = train_df.copy()\n",
    "    test_t = test_df.copy()\n",
    "    train_t[cols] = scaler.transform(X_train)\n",
    "    test_t[cols] = scaler.transform(X_test)\n",
    "    scaler_path = SCALER_DIR / f\"{subset_name}_global_scaler.pkl\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Saved global scaler to {scaler_path}\")\n",
    "    return train_t, test_t, scaler\n",
    "\n",
    "\n",
    "def conditional_standardize(train_df: pd.DataFrame, test_df: pd.DataFrame, n_clusters: int = 6, subset_name: str = \"FD\"):\n",
    "    \"\"\"Cluster units by their operating settings (mean across cycles) and standardize sensor+op features per-cluster.\n",
    "\n",
    "    Strategy:\n",
    "    - Compute per-unit mean of OP_COLS across cycles\n",
    "    - Fit KMeans on these per-unit means (n_clusters default 6)\n",
    "    - Assign cluster labels to each row (by unit)\n",
    "    - For each cluster, fit StandardScaler on train rows in that cluster and transform both train/test rows in that cluster\n",
    "\n",
    "    Returns (train_t, test_t, dict_of_scalers, unit_cluster_map)\n",
    "    \"\"\"\n",
    "    # compute unit-level op means\n",
    "    unit_means = train_df.groupby(\"unit\")[OP_COLS].mean().reset_index()\n",
    "    X = unit_means[OP_COLS].values\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    unit_means[\"cluster\"] = labels\n",
    "\n",
    "    # map unit -> cluster\n",
    "    unit_cluster = dict(zip(unit_means[\"unit\"], unit_means[\"cluster\"]))\n",
    "\n",
    "    # create copies\n",
    "    train_t = train_df.copy()\n",
    "    test_t = test_df.copy()\n",
    "\n",
    "    scalers = {}\n",
    "\n",
    "    # assign cluster column\n",
    "    train_t[\"cluster\"] = train_t[\"unit\"].map(unit_cluster)\n",
    "    test_t[\"cluster\"] = test_t[\"unit\"].map(unit_cluster)\n",
    "\n",
    "    cols = SENSOR_COLS + OP_COLS\n",
    "\n",
    "    for c in sorted(train_t[\"cluster\"].unique()):\n",
    "        mask_train = train_t[\"cluster\"] == c\n",
    "        mask_test = test_t[\"cluster\"] == c\n",
    "        Xc_train = train_t.loc[mask_train, cols]\n",
    "        # If cluster has no train rows (very unlikely), skip\n",
    "        if Xc_train.shape[0] < 2:\n",
    "            # fallback to global scaler behavior for this cluster\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_df[cols].values)\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(Xc_train.values)\n",
    "        # transform\n",
    "        if mask_train.sum() > 0:\n",
    "            train_t.loc[mask_train, cols] = scaler.transform(Xc_train.values)\n",
    "        if mask_test.sum() > 0:\n",
    "            Xc_test = test_t.loc[mask_test, cols]\n",
    "            test_t.loc[mask_test, cols] = scaler.transform(Xc_test.values)\n",
    "        scalers[c] = scaler\n",
    "        joblib.dump(scaler, SCALER_DIR / f\"{subset_name}_cluster_{c}_scaler.pkl\")\n",
    "\n",
    "    # also save unit->cluster mapping\n",
    "    joblib.dump(unit_cluster, SCALER_DIR / f\"{subset_name}_unit_cluster_map.pkl\")\n",
    "    print(f\"Saved {len(scalers)} cluster scalers and unit->cluster map for {subset_name}\")\n",
    "    return train_t, test_t, scalers, unit_cluster\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# EDA / Visualization\n",
    "# ------------------------\n",
    "\n",
    "def summary_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute summary statistics for sensor columns (mean, std, min, max)\"\"\"\n",
    "    stats = df[SENSOR_COLS].agg([\"mean\", \"std\", \"min\", \"max\"]).T\n",
    "    stats.columns = [\"mean\", \"std\", \"min\", \"max\"]\n",
    "    return stats\n",
    "\n",
    "\n",
    "def plot_unit_sensors(df: pd.DataFrame, unit_id: int, sensors: list = None, nrows: int = None):\n",
    "    \"\"\"Plot time-series for a given unit for selected sensors.\"\"\"\n",
    "    if sensors is None:\n",
    "        sensors = SENSOR_COLS[:6]\n",
    "    unit_df = df[df[\"unit\"] == unit_id]\n",
    "    n = len(sensors)\n",
    "    if nrows is None:\n",
    "        nrows = int(np.ceil(n / 2))\n",
    "    plt.figure(figsize=(12, 3 * nrows))\n",
    "    for i, s in enumerate(sensors, 1):\n",
    "        plt.subplot(nrows, 2, i)\n",
    "        plt.plot(unit_df[\"cycle\"], unit_df[s])\n",
    "        plt.title(f\"Unit {unit_id} - {s}\")\n",
    "        plt.xlabel(\"cycle\")\n",
    "        plt.ylabel(s)\n",
    "        plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sensor_over_units(df: pd.DataFrame, sensor: str, sample_units: list = None, ncols: int = 2):\n",
    "    \"\"\"Overlay sensor traces from several units to see variability.\"\"\"\n",
    "    if sample_units is None:\n",
    "        sample_units = df[\"unit\"].unique()[:6]\n",
    "    n = len(sample_units)\n",
    "    plt.figure(figsize=(12, 3 * int(np.ceil(n / ncols))))\n",
    "    for i, u in enumerate(sample_units, 1):\n",
    "        plt.subplot(int(np.ceil(n / ncols)), ncols, i)\n",
    "        tmp = df[df[\"unit\"] == u]\n",
    "        plt.plot(tmp[\"cycle\"], tmp[sensor])\n",
    "        plt.title(f\"Unit {u}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame, sensors: list = None):\n",
    "    if sensors is None:\n",
    "        sensors = SENSOR_COLS[:12]\n",
    "    corr = df[sensors].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=False, cmap=\"vlag\")\n",
    "    plt.title(\"Sensor correlation matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Top-level processing per subset\n",
    "# ------------------------\n",
    "\n",
    "def process_subset(subset: str, raw_base: Path = RAW_BASE, processed_base: Path = PROCESSED_BASE, n_clusters: int = 6):\n",
    "    \"\"\"Load, compute RUL, normalize according to NORMALIZATION_MAP, run basic EDA plots, and save CSVs/scalers.\n",
    "\n",
    "    subset: e.g., \"FD001\"\n",
    "    Expects files: train_FD001.txt, test_FD001.txt, RUL_FD001.txt\n",
    "    \"\"\"\n",
    "    subset = subset.upper()\n",
    "    print(f\"Processing {subset} ...\")\n",
    "    train_f = raw_base / f\"train_{subset}.txt\"\n",
    "    test_f = raw_base / f\"test_{subset}.txt\"\n",
    "    rul_f = raw_base / f\"RUL_{subset}.txt\"\n",
    "\n",
    "    if not (train_f.exists() and test_f.exists() and rul_f.exists()):\n",
    "        raise FileNotFoundError(f\"Missing files for {subset} in {raw_base}. Expected {train_f}, {test_f}, {rul_f}.\")\n",
    "\n",
    "    train_df = read_cmapps_file(train_f)\n",
    "    test_df = read_cmapps_file(test_f)\n",
    "    rul_series = read_rul_file(rul_f)\n",
    "\n",
    "    train_df = compute_train_rul(train_df)\n",
    "    test_df = compute_test_rul(test_df, rul_series)\n",
    "\n",
    "    # Basic EDA: show shape and sensor stats\n",
    "    print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    stats = summary_stats(train_df)\n",
    "    print(\"Sensor summary stats (train):\")\n",
    "    print(stats.head())\n",
    "\n",
    "    # Choose normalization\n",
    "    norm = NORMALIZATION_MAP.get(subset, \"global\")\n",
    "    if norm == \"global\":\n",
    "        train_t, test_t, scaler = global_standardize(train_df, test_df, cols=None, subset_name=subset)\n",
    "    else:\n",
    "        train_t, test_t, scalers, unit_cluster_map = conditional_standardize(train_df, test_df, n_clusters=n_clusters, subset_name=subset)\n",
    "\n",
    "    # Save processed\n",
    "    processed_dir = processed_base / subset\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_out = processed_dir / f\"train_{subset}.csv\"\n",
    "    test_out = processed_dir / f\"test_{subset}.csv\"\n",
    "    train_t.to_csv(train_out, index=False)\n",
    "    test_t.to_csv(test_out, index=False)\n",
    "    print(f\"Saved processed files: {train_out}, {test_out}\")\n",
    "\n",
    "    # Quick plots (show first unit)\n",
    "    first_unit = train_df[\"unit\"].unique()[0]\n",
    "    try:\n",
    "        plot_unit_sensors(train_df, first_unit, sensors=SENSOR_COLS[:6])\n",
    "        plot_correlation_heatmap(train_df)\n",
    "    except Exception as e:\n",
    "        print(\"Plotting failed:\", e)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_t,\n",
    "        \"test\": test_t,\n",
    "        \"stats\": stats,\n",
    "        \"normalization\": norm,\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helper: run pipeline for all FD001-FD004\n",
    "# ------------------------\n",
    "\n",
    "def run_all(subsets=None):\n",
    "    if subsets is None:\n",
    "        subsets = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "    results = {}\n",
    "    for s in subsets:\n",
    "        results[s] = process_subset(s)\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI entrypoint\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"CMAPSS ingestion & preprocess pipeline (FD001-FD004)\")\n",
    "    parser.add_argument(\"--raw_dir\", type=str, default=str(RAW_BASE), help=\"path to raw CMAPSS files\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=str(PROCESSED_BASE), help=\"path to save processed csvs\")\n",
    "    parser.add_argument(\"--subsets\", type=str, default=\"FD001,FD002,FD003,FD004\", help=\"comma-separated subsets\")\n",
    "    parser.add_argument(\"--clusters\", type=int, default=6, help=\"n clusters for conditional normalization\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    RAW_BASE = Path(args.raw_dir)\n",
    "    PROCESSED_BASE = Path(args.out_dir)\n",
    "    PROCESSED_BASE.mkdir(parents=True, exist_ok=True)\n",
    "    SCALER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    subsets = [s.strip().upper() for s in args.subsets.split(\",\")]\n",
    "    run_all(subsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633fdc-afce-46b3-b554-1dfc997f1ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
